{
 "metadata": {
  "name": "",
  "signature": "sha256:dce9e159d98d75e53caeeeb81ff17999a67bc072c7fd1000ef4f8799fc771dbc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Baseline algorithms for SPAM filtering"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import sklearn\n",
      "import numpy as np\n",
      "\n",
      "# Classifiers\n",
      "from sklearn.naive_bayes import  MultinomialNB, BernoulliNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier \n",
      "\n",
      "# Evaluation\n",
      "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
      "from sklearn.cross_validation import train_test_split, StratifiedKFold  \n",
      "from sklearn.grid_search import GridSearchCV"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "1. Naive Bayes family"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll be using scikit-learn here, choosing the family of [Naive Bayes](http://en.wikipedia.org/wiki/Naive_Bayes_classifier) classifiers to start with. We omit the Gaussian Classifier as it is not well suited for discrete features like we have in this case."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Load Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Naive Bayes algorithms can handle categorical data directly, so we omit the feature encoding step when loading data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from dataset import load_or_create_dataset\n",
      "X_train, X_test, y_train, y_test = load_or_create_dataset(encode_cats=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "need more than 2 values to unpack",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-16-570e1b7d72c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_or_create_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_or_create_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencode_cats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mValueError\u001b[0m: need more than 2 values to unpack"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can visualize our dataset to have an intuition of its distribution, outliers, etc."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "\n",
      "# instantiate the model\n",
      "model = PCA(n_components=2)\n",
      "\n",
      "# fit the model: notice we don't pass the labels!\n",
      "X = np.vstack((X_train, X_test))\n",
      "y = np.append(y_train, y_test)\n",
      "model.fit(X)\n",
      "\n",
      "# transform the data to two dimensions\n",
      "X_PCA = model.transform(X)\n",
      "print (\"shape of result:\", X_PCA.shape)\n",
      "\n",
      "# plot the results along with the labels\n",
      "fig, ax = plt.subplots()\n",
      "im = ax.scatter(X_PCA[:, 0],\n",
      "                X_PCA[:, 1],\n",
      "#                X_PCA[:, 2],\n",
      "                c=y)\n",
      "fig.colorbar(im);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1.1 MultinomialNB"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spam_detector = MultinomialNB().fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = spam_detector.predict(X_test)\n",
      "plt.matshow(confusion_matrix(y_test, y_pred), cmap=plt.cm.binary, interpolation='nearest')\n",
      "plt.title('confusion matrix')\n",
      "plt.colorbar()\n",
      "plt.ylabel('expected label')\n",
      "plt.xlabel('predicted label')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From this confusion matrix, we can compute precision and recall, or their combination (harmonic mean) F1:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print (classification_report(y_test, y_pred))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1.3 BernoulliNB"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before we move on, we define a function that synthesizes the process of training, testing, evaluating and printing out a report (confusion matrix and scores)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def evaluate_and_report(classifier):\n",
      "    spam_detector = classifier.fit(X_train, y_train)\n",
      "    y_pred = spam_detector.predict(X_test)\n",
      "\n",
      "    plt.matshow(confusion_matrix(y_test, y_pred), cmap=plt.cm.binary, interpolation='nearest')\n",
      "    plt.title('confusion matrix')\n",
      "    plt.colorbar()\n",
      "    plt.ylabel('expected label')\n",
      "    plt.xlabel('predicted label')\n",
      "\n",
      "    print (classification_report(y_test, y_pred))\n",
      "    print (\"accuracy: %.3f\" % accuracy_score(y_test, y_pred))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evaluate_and_report(BernoulliNB())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Removing categorical features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Categorical features encoded (via OneHotEncoder) have very different probability distributions, and thus generally don't behave well combined with other features in probabilistic classifiers like the Naive Bayes family.\n",
      "\n",
      "We run our experiments again without including categorical features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, y_train, y_test = load_or_create_dataset(use_cats=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evaluate_and_report(MultinomialNB())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evaluate_and_report(BernoulliNB())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2. SVM classifier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try with another classifier: [Support Vector Machines (SVM)](http://en.wikipedia.org/wiki/Support_vector_machine). \n",
      "\n",
      "SVMs are a great starting point when classifying text data, getting state of the art results very quickly and with pleasantly little tuning (although a bit more than Naive Bayes).\n",
      "\n",
      "Before we start we reload a scaled dataset with categorical features:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, y_train, y_test = load_or_create_dataset(scaled=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Parameters for the following examples have been tuned separately using the GridCV module of scikit-learn.\n",
      "We omit those here to keep the report brief, the code for that be seen on files grid_svm.py, grid_dtree.py and grid_rdf.py from the repo.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf =  SVC(kernel='linear', C=1, class_weight= {1: 3})\n",
      "evaluate_and_report(clf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = SVC(kernel='rbf', gamma=150, class_weight={1: 5})\n",
      "evaluate_and_report(clf)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3. Decision Trees and Random Forests"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "params = {'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 75, 'max_depth': 20}\n",
      "clf = DecisionTreeClassifier(**params)\n",
      "evaluate_and_report(clf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "params = {'min_samples_split': 128, 'min_samples_leaf': 5, 'max_features': 100, 'max_depth': 10}\n",
      "clf = DecisionTreeClassifier(**params)\n",
      "evaluate_and_report(clf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import  RandomForestClassifier\n",
      "params = {'max_features': 150, 'n_estimators': 5, 'max_depth': 5}\n",
      "\n",
      "clf = RandomForestClassifier(**params)\n",
      "evaluate_and_report(clf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Some observations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In terms of reducing the number of false positives, we must look for models that show high recall for ham and high precision for SPAM.\n",
      "\n",
      "Among the models analyzed so far the best seems to be:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf =  SVC(kernel='linear', C=1, class_weight= {1: 3})\n",
      "evaluate_and_report(clf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The problem with this one is its relatively low recall score for spam messages, which implies it is only detecting 11% of true SPAM messages.\n",
      "\n",
      "To seek for a better balance between spam precision and recall we must look at f1-score.\n",
      "\n",
      "Some models looking more promising from that perspective are:"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Random Forest"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, y_train, y_test = load_or_create_dataset(use_cats=True)\n",
      "\n",
      "params = {'max_features': 150, 'n_estimators': 4, 'max_depth': 8.75}\n",
      "clf = RandomForestClassifier(**params)\n",
      "evaluate_and_report(clf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We try to improve performance applying feature selection"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.feature_importances_\n",
      "X_train, X_test = [clf.transform(x) for x in [X_train, X_test]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We perform a new parameter search for the reduced feature set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "params = dict(\n",
      "    max_depth=np.linspace(5,10,5),\n",
      "    n_estimators=[3, 4, 5, 10],\n",
      "    max_features = [5, 10, 15, 18]\n",
      ")\n",
      "\n",
      "clf = GridSearchCV(\n",
      "    RandomForestClassifier(),  \n",
      "    param_grid=params,  # parameters to tune via cross validation\n",
      "    refit=True,  # fit using all data, on the best detected classifier\n",
      "    n_jobs=-1,  # number of cores to use for parallelization; -1 for \"all cores\"\n",
      "    scoring='precision',  # what score are we optimizing?\n",
      "    cv=StratifiedKFold(y_train, n_folds=3),  # what type of cross validation to use\n",
      ")\n",
      "\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "print(\"Best parameters set found on training set:\")\n",
      "print(clf.best_params_)\n",
      "\n",
      "y_true, y_pred = y_test, clf.predict(X_test)\n",
      "print(classification_report(y_true, y_pred))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Multinomial Naive Bayes without categorical feature encoding"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, y_train, y_test = load_or_create_dataset(use_cats=False)\n",
      "evaluate_and_report(BernoulliNB())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}